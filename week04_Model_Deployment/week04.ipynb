{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a204b164",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "![screenshot01](Screenshot_01.png)\n",
    "\n",
    "* Until now, we have been working on the first (Design) and the second (Training) step\n",
    "* When it comes to model deployment, we hava multiple options.\n",
    "* Do we need the predictions immediately? Or can it wait a bit?\n",
    "    * If we can wait, we apply the model regularly; this in the so called *Batch Mode* or *Offline Mode*\n",
    "    * If we need the predictions immediately, we run the model in the so called *Online Mode. In this case we have two options to make the model available:\n",
    "       1. Through a webservice\n",
    "       2. Through streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b1fdb",
   "metadata": {},
   "source": [
    "## Batch Mode\n",
    "\n",
    "* Apply the model regularly, e.g. every 10 minutes, every day, etc.\n",
    "* Often used for marketing related tasks\n",
    "![batch_mode](batch_mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6b13f",
   "metadata": {},
   "source": [
    "## Web Services\n",
    "* Common way of deploying models\n",
    "![webservice](webservice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bbdae",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "* In contrast to a web service multiple consumers can use the streaming output\n",
    "![streaming](streaming.png)\n",
    "* Example: \n",
    "    * Uploading of a Video \n",
    "    * is tested by different services (e.g. violence, copyright, ...) \n",
    "    * these services send their predictions to a decision service if the video can be uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d2fe5",
   "metadata": {},
   "source": [
    "# Web-services: Deploying models with Flask and Docker\n",
    "\n",
    "* Use the model we created in the previous weeks and deploy it via a webservice\n",
    "    * Create a virtual environment\n",
    "    * Create a script for predicting\n",
    "    * Put the script into a flask app\n",
    "    * Package the app to docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f2190",
   "metadata": {},
   "source": [
    "* First find the exact version of sklearn we used to create the model. If we load it using another version it might not work. We can do that using ```pip freeze | grep scikit-learn```. Or in the conda environment ```conda list | grep scikit-learn``` shows ```scikit-learn 1.0.2```\n",
    "* Create a virtual environemt: ```pipenv install scikitlearn==1.0.2 flask --python 3.9```\n",
    "* Start the environment: ```pipenv shell```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a264c8",
   "metadata": {},
   "source": [
    "* Now create the prediction script: ```predict.py```\n",
    "* Test it with ```test.py```, where the model is applied to a specific example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f58062",
   "metadata": {},
   "source": [
    "* Now turn it into a flask application: ```predict_flask.py```\n",
    "* Test this using ```test_flask.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbcc04a",
   "metadata": {},
   "source": [
    "* When we start the app using ```python3 predict_flask.py```, we get the following warning:\n",
    "```WARNING: This is a development server. Do not use it in a production deployment.\n",
    "   Use a production WSGI server instead.```\n",
    "* To fix this use ```gunicorn```: ```pipenv install gunicorn```\n",
    "* Flask is only used to run things locally\n",
    "* start the app with gunicorn: ```gunicorn --bind=0.0.0.0:9696 predict_flask:app```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1d1a3",
   "metadata": {},
   "source": [
    "* Note: The library ```requests``` is included in the base python, but not in our virtual environment. We need this library only for testing. I.e. we can install it as a development dependency: ```pipenv install --dev requests```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5fb3e",
   "metadata": {},
   "source": [
    "* Now package everything into docker: ```Dockerfile```\n",
    "* Build the image: ```docker build -t ride-duration-prediction-service:v1 .```\n",
    "* Run the imgae: ```docker run -it --rm -p 9696:9696 ride-duration-prediction-service:v1```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01501c67",
   "metadata": {},
   "source": [
    "## Web Services: Getting the models from the model registry (MLflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f831507",
   "metadata": {},
   "source": [
    "* Use the model from random-forest.ipynb\n",
    "* Use the runid from mlflow\n",
    "* Adapt the ```predict_flask.py``` script\n",
    "* Use the run id and pyfunc to load the model\n",
    "* The dict vectorizer is stored as an artifact, to load is we need to use the client\n",
    "* Better: define dict vectorizer and model as pipeline and use pyfunc to load both together. Adapt test_flask.py accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d04b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc45095",
   "metadata": {},
   "source": [
    "# How to monitor ML Models in Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c2808",
   "metadata": {},
   "source": [
    "* The model quality during production may be effected by the changes in the input data (e.g data processing isuues, problems with the data source, ...)\n",
    "* The model in production is constantly receiving new data. However, this data might have a different probability distribution than the one you have trained the model. Using the original model with the new data distribution will cause a drop in model performance.\n",
    "\n",
    "**Data Drift**\n",
    "\n",
    "Data drift is the situation where the model’s input distribution changes.\n",
    "\n",
    "$Pt1 (X) ≠ Pt2 (X)$\n",
    "\n",
    "**Concept Drift**\n",
    "\n",
    "To know what concept drift is, we need a definition of “concept”. Concept stands for the joint probability distribution of a machine learning model’s inputs (X) and outputs (Y). We can express their relationship in the following form:\n",
    "\n",
    "$P(X, Y) = P(Y) P(X|Y) = P(X) P(Y|X)$\n",
    "\n",
    "Concept shift happens when the joint distribution of inputs and outputs changes:\n",
    "\n",
    "$Pt1 (X, Y) ≠ Pt2 (X, Y)$\n",
    "\n",
    "Concept drift can originate from any of the concept components. The most important source is the posterior class probability $P(Y|X)$ , as it shows how well our model understands the relationship between inputs and outputs. For this reason, people use the term “concept drift” or “real concept drift” for this specific type.\n",
    "\n",
    "\n",
    "More details: https://deepchecks.com/data-drift-vs-concept-drift-what-are-the-main-differences/\n",
    "\n",
    "When this happens we have to intervene, e.g. retrain the model, fallback to some robust system, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a08e1",
   "metadata": {},
   "source": [
    "![things to monitor](monitoring_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66caf7",
   "metadata": {},
   "source": [
    "### Batch vs. Online serving Models\n",
    "**Online**\n",
    "* Online monitoring\n",
    "* If no real-time update of the model is necessary, batch monitoring may be used for online mode aswell\n",
    "\n",
    "**Batch**\n",
    "* Batch monitoring\n",
    "* Most ML models in production operate in batch mode, i.e. the pipeline my use prefect or airflow\n",
    "* You may add a block of calculations after specific step of the pipeline and run some checks if the data and model behaves as expected. I.g. calculate some metrics, save them into a database and visualize them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7cf14",
   "metadata": {},
   "source": [
    "This is what we want to implement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437598f",
   "metadata": {},
   "source": [
    "![pipeline](monitoring_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49dc795",
   "metadata": {},
   "source": [
    "We will continue with the New York taxi ride example. \n",
    "\n",
    "We will use MongoDB for logging\n",
    "* MongoDB is a NoSQL database, we can push our data that may have different number of fields\n",
    "* Our data is in json format (unstructured)\n",
    "* In order to push our data to MongoDB, we need to connect to Host, create a database, create a collection and push our data there\n",
    "\n",
    "We need to implement online monitoring\n",
    "* we need to send our data from the prediction service to the monitoring service\n",
    "* This calculates the metrics and saves them into a Prometheus database\n",
    "* The Prometheus database shares the data with the Grafana service\n",
    "* Both prometeus and Grafana are open source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3b3ed",
   "metadata": {},
   "source": [
    "We can the additionally include batch monitoring\n",
    "![batch monitoring](monitoring_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600f298",
   "metadata": {},
   "source": [
    "* Include a prefect flow, that collects data from MongoDB and calculates metrics\n",
    "* From that we can create a visual report in html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3d387",
   "metadata": {},
   "source": [
    "We will use docker to combine all this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9bc52",
   "metadata": {},
   "source": [
    "### Example: New York Taxi data\n",
    "* Our example (```predict_flask.py```) uses Flask to integrate the service\n",
    "* The prediction is returned in a json script\n",
    "* Update this service with two more instructions:\n",
    "    * logging\n",
    "    * send data to monitoring service\n",
    "* We call the new file ```prediction_service/app.py``` and save it in ```prediction_service```\n",
    "* We use the ```evidently service``` to calculate metrics and store them into prometheus database \n",
    "* More:\n",
    "    * https://evidentlyai.com/ \n",
    "    * https://github.com/evidentlyai/evidently/tree/main/examples/integrations/grafana_monitoring_service\n",
    "    \n",
    "#### Real-time monitoring\n",
    "\n",
    "* This monitoring service needs to be customized. \n",
    "* ```/evidently_service/app.py```\n",
    "    * Flask service, which uses some modules from evidently service\n",
    "    * create a Flask application, that takes the data sent from the prediction service, calculate some metrics, with help of evidently monitor these, and stores these metrics into prometheus database.\n",
    "    * the ```iterate``` function describes how the metrics are calculated\n",
    "        * the ```window size``` is specified: A lot of metrics cannot be calculated based on just 2 event, but we need a batch of events. The ```window size``` specifies this batch size. The results are sent to prometheus.\n",
    "        \n",
    "* This is the workflow we implement:\n",
    "    * Simulate the production with a python script\n",
    "    * This script sends some request to the prediction service (send_data.py)\n",
    "        * send_data.py generates predictions and saves them into a mongobd.\n",
    "        * to see how to access this data and how the data looks like refer to pymongo_example.ipynb\n",
    "    * The prediction service saves the logs and sends request to the monitoring service\n",
    "* We can then observe the monitored metrics in grafana (docker is always running!):\n",
    "    * localhost:3000\n",
    "    * login: admin, pw: admin\n",
    "    ![grafana](grafana.png)\n",
    "* The stored data can be seen in the prometheus database: localhost:9091\n",
    "    * We can access the entries via the execution line, e.g.:```evidently:data_drift:p_value```\n",
    "    ![prometheus](prometheus.png)\n",
    "    \n",
    "#### Batch Monitoring\n",
    "\n",
    "* Often it is necessary to calculate metrics over a batch of data\n",
    "* Use the data we stored to the MongoDB to create the batch-report using prefect\n",
    "* Previously we used prefect to run a model and apply it to a (new) batch of data, here we will use it to create a dashboard\n",
    "* We use the script ```prefect_example.py```, where we will use evidently to create a dashboard and a (json-)report:\n",
    "    ```\n",
    "    from evidently.dashboard import Dashboard\n",
    "    from evidently.model_profile import Profile\n",
    "    ```\n",
    "* tasks\n",
    "    * ```upload_traget```: adds new data to MongoDB (as reference data, these was not necessary in online monitoring)\n",
    "    * ```load_reference_data```: makes predictions of reference data\n",
    "    * ```fetch_data```: loads current data from MongoDB\n",
    "    * ```run_evidently```: prepares profile\n",
    "    * ```save_report```\n",
    "    * ```save_html_report```\n",
    "* the flow runs these tasks\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071701c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
